# General Overview

This project demonstrates a **real-time data pipeline** that continuously updates a data lake with changes from a transactional database using Change Data Capture (CDC).

Specifically, CDC events captured from a SQL database via Debezium are streamed into Kafka. Apache Flink consumes these events from Kafka to perform transformation, filtering, and enrichment of the raw change data.

The processed data is written as partitioned **Parquet** files to an S3-compatible data lake bucket, enabling efficient storage and querying.

Key features implemented include:

- Continuous, low-latency ingestion of CDC events from Kafka
- Exactly-once processing semantics with Flink checkpointing
- Event-time processing with watermarks to handle out-of-order and late events
- Writing partitioned Parquet files directly to S3-compatible storage

This pipeline provides a modern streaming alternative to traditional batch ETL jobs, enabling fresher data availability and faster analytics.

## Infrastructure Components

This Docker Compose stack brings up the following services:

- **MySQL**
  A MySQL 8.0 database running on port 3306, initialized with an `inventory.orders` table and a `debezium` user. Includes a healthcheck to wait until the database is ready for connections.
- **Kafka (KRaft mode)**
  A single-node Kafka broker in KRaft (ZooKeeper-less) mode:

  - **PLAINTEXT** listener on port 9092 for host-side clients
  - **PLAINTEXT_INTERNAL** listener on port 29092 for intra-Docker traffic
    Automatically creates an `orders_cdc` topic (3 partitions, RF=1) and includes a broker healthcheck.
- **Kafka Connect (Debezium)**
  Debezium’s Kafka Connect image on port 8083, configured to consume MySQL binlog events and publish CDC records to Kafka topics. Waits for both MySQL and Kafka to be healthy before starting, and exposes a REST endpoint for connector management.
- **Connector Initializer**
  A lightweight container that waits for the Connect REST API, then registers the MySQL-to-Kafka connector using `register-orders-connector.json`. Runs once at startup to wire up the CDC pipeline.

## Repository Layout

* **infra/** – Docker Compose, MySQL init, connector config
* **scripts/** – simulate-db-changes.sh helper
* **src/** – Java source (config, model, serde, sink, job)
* **config.toml** – pipeline settings

## Prerequisites

* Docker & Docker Compose
* Java 17 & Maven

## Running Locally

This project includes a `Makefile` to simplify the most common Docker Compose workflows using Docker Compose profiles.

### Makefile Profiles and Docker Compose
The `Makefile` uses Docker Compose with a specific profile for the application services:

- `PROFILE = --profile app` is set to selectively run containers tagged with the `app` profile in `docker-compose.yml`.

- `make infra-up` runs only the core infrastructure services (MySQL, Kafka, Connect, connector-init) without the `app` profile, starting them detached.

- `make full-up` first builds images and then runs the full stack — infrastructure plus the Flink job and related services — with the `app` profile enabled.

- `make down` tears down all services and volumes.

Example:
```bash
make infra-up       # starts only infra services
make full-up        # builds and starts full infra + app services
make down           # stops and removes all containers and volumes
```

### IDE (IntelliJ)

1. **Start infra services only**

   ```bash
   make infra-up
   ```
2. **Create or select your Run configuration**
   1. Open **Run -> Edit Configurations…**
   2. Create (or select) an **Application** config.
   3. Set Main class to `com.example.flinkcdc.OrdersCdcToParquetJob`
   4. Under **Modify options** dropdown, Check **Add dependencies with "provided" scope to classpath**.
   5. Hit **Run** - this spins up Flink’s in‑process local cluster.
3. **Tear Down**

### Full stack (Docker Compose)
1. **Build and start full stack**
  
  ```bash
  make full-up
  ```
2. **Verify**
* Flink UI: http://localhost:8081

3. **Tear Down**
```bash
make down
```

## Configuration

Edit `config.toml` or point at a file via:

```bash
export CONFIG_FILE=/path/to/config.toml
```

## Usage
After starting the stack and running the Flink job, the following commands help verify the data pipeline:

* Tail the `orders_cdc` Kafka topic to monitor incoming CDC events in real time:

```bash
docker exec kafka kafka-console-consumer.sh \
  --bootstrap-server kafka:29092 \
  --topic orders_cdc
```

* Inspect the Parquet files generated by the Flink job in the local data lake directory (adjust the date and path as needed):

```bash
ls /tmp/orders/order_date=2025-07-19/
parquet-tools head /tmp/orders/order_date=2025-07-19/part-*.parquet
```
> **Note:** The `parquet-tools` utility must be installed on your system to inspect Parquet files.

# Next steps

- Handle deletes with Iceberg/Hudi or tombstone logic
- Extend Flink job with windowed aggregations
- Add Prometheus/Grafana dashboards for metrics and alerts
- Add ci for code formatting